"""
å¿«é€Ÿè°ƒä¼˜è„šæœ¬ - åªè·‘æ’å€¼+åŒ¹é…å±‚ï¼Œå¤ç”¨é¢„å¤„ç†ç»“æœ

å·¥ç¨‹åŒ–è®¾è®¡ï¼š
1. é¢„å¤„ç†ç»“æœç¼“å­˜åœ¨ our_ans/preprocessed_cache/<dataset_name>/
2. åªéœ€è¦è·‘ä¸€æ¬¡é¢„å¤„ç†ï¼Œåç»­æ‰€æœ‰è°ƒå‚å®éªŒéƒ½å¤ç”¨
3. å•æ¬¡å®éªŒæ—¶é—´ä»å‡ åˆ†é’Ÿé™åˆ°å‡ åç§’

ç”¨æ³•ï¼š
    # ç¬¬ä¸€æ¬¡ï¼šç”Ÿæˆé¢„å¤„ç†ç¼“å­˜
    python fast_eval_v3_4.py --prepare validation
    
    # å¿«é€Ÿè°ƒå‚ï¼ˆåªè·‘æ’å€¼+åŒ¹é…ï¼‰
    python fast_eval_v3_4.py --dataset validation --update_distance 2500
    python fast_eval_v3_4.py --dataset validation --update_distance 2500 --update_angle 70 --iteration_rounds 7
    
    # æ‰¹é‡æµ‹è¯•
    python fast_eval_v3_4.py --dataset validation --grid update_distance=2000,2500,3000
"""

import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
import pandas as pd

sys.path.append('.')

from common.ablation_wrapper import preprocess_with_params, interpolation_with_params
from common.è¯„æµ‹è„šæœ¬ import evaluate


# v3.4 çš„åŸºå‡†é…ç½®
BASELINE_CONFIG = {
    # é¢„å¤„ç†å‚æ•°
    'safe_distance': 200,
    'esm_angle_threshold': 0.065,
    'esm_speed_threshold': 1000,
    'esm_check_distance': 200000,
    'enable_deduplication': False,
    'enable_esm_merge': True,
    
    # æ’å€¼+åŒ¹é…å‚æ•°ï¼ˆè¿™äº›æ˜¯å¿«é€Ÿè°ƒä¼˜çš„ç›®æ ‡ï¼‰
    'update_distance': 2000,
    'update_speed': 1000,
    'update_angle': 60,
    'ransac_distance': 500,
    'ransac_min_points': 5,
    'iteration_rounds': 5,
    'enable_ransac': True,
    'interpolation_method': 'akima',
    'enable_kinematic_constraints': True,
    'enable_outlier_detection': True,
}


def get_dataset_paths(dataset_name):
    """è·å–æ•°æ®é›†è·¯å¾„"""
    if dataset_name == 'validation':
        return {
            'radar_file': 'validation_set/radar_detection.csv',
            'sensor_file': 'official/20_ans/sensors.txt',
            'gt_dir': 'validation_set',
        }
    elif dataset_name == 'final':
        return {
            'radar_file': 'official/radar_detection.csv',
            'sensor_file': 'official/20_ans/sensors.txt',
            'gt_dir': None,
        }
    else:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")


def prepare_cache(dataset_name, config=None):
    """
    ç”Ÿæˆé¢„å¤„ç†ç¼“å­˜

    è¿™ä¸€æ­¥æ¯”è¾ƒæ…¢ï¼ˆå‡ åç§’åˆ°å‡ åˆ†é’Ÿï¼‰ï¼Œä½†åªéœ€è¦è·‘ä¸€æ¬¡
    """
    if config is None:
        config = BASELINE_CONFIG

    paths = get_dataset_paths(dataset_name)
    cache_dir = Path('preprocessed_cache') / dataset_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print(f"ğŸ”§ ç”Ÿæˆé¢„å¤„ç†ç¼“å­˜: {dataset_name}")
    print("="*80)
    print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    print("\nè¿™ä¸€æ­¥æ¯”è¾ƒæ…¢ï¼Œä½†åªéœ€è¦è·‘ä¸€æ¬¡...")
    
    # è¿è¡Œé¢„å¤„ç†
    preprocess_with_params(
        paths['radar_file'],
        paths['sensor_file'],
        str(cache_dir),
        safe_distance=config['safe_distance'],
        esm_angle_threshold=config['esm_angle_threshold'],
        esm_speed_threshold=config['esm_speed_threshold'],
        esm_check_distance=config['esm_check_distance'],
        enable_deduplication=config['enable_deduplication'],
        enable_esm_merge=config['enable_esm_merge']
    )
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    cache_info = {
        'dataset': dataset_name,
        'created_at': datetime.now().isoformat(),
        'preprocess_config': {
            'safe_distance': config['safe_distance'],
            'esm_angle_threshold': config['esm_angle_threshold'],
            'esm_speed_threshold': config['esm_speed_threshold'],
            'esm_check_distance': config['esm_check_distance'],
            'enable_deduplication': config['enable_deduplication'],
            'enable_esm_merge': config['enable_esm_merge'],
        }
    }
    
    with open(cache_dir / 'cache_info.json', 'w', encoding='utf-8') as f:
        json.dump(cache_info, f, indent=2, ensure_ascii=False)
    
    print("\nâœ… é¢„å¤„ç†ç¼“å­˜ç”Ÿæˆå®Œæˆï¼")
    print(f"\nç¼“å­˜æ–‡ä»¶:")
    print(f"  - {cache_dir / 'located_ESM_points_updated.csv'}")
    print(f"  - {cache_dir / 'deduplacate_radar_points_updated.csv'}")
    print(f"\nç°åœ¨å¯ä»¥å¿«é€Ÿè°ƒå‚äº†ï¼")
    
    return cache_dir


def fast_eval(dataset_name, config_override, description=""):
    """
    å¿«é€Ÿè¯„ä¼° - åªè·‘æ’å€¼+åŒ¹é…å±‚ï¼ˆä¸²è¡Œç‰ˆæœ¬ï¼‰

    Args:
        dataset_name: æ•°æ®é›†åç§°
        config_override: é…ç½®è¦†ç›–
        description: å®éªŒæè¿°

    è¿™ä¸€æ­¥å¾ˆå¿«ï¼ˆå‡ åç§’ï¼‰ï¼Œå¯ä»¥åå¤è°ƒå‚
    """
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
    cache_dir = Path('preprocessed_cache') / dataset_name
    located_file = cache_dir / 'located_ESM_points_updated.csv'
    radar_file = cache_dir / 'deduplacate_radar_points_updated.csv'
    
    if not located_file.exists() or not radar_file.exists():
        print(f"âŒ é¢„å¤„ç†ç¼“å­˜ä¸å­˜åœ¨: {cache_dir}")
        print(f"\nè¯·å…ˆè¿è¡Œ: python fast_eval_v3_4.py --prepare {dataset_name}")
        return None, None
    
    # åˆå¹¶é…ç½®
    config = BASELINE_CONFIG.copy()
    config.update(config_override)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = Path('fast_tune_results') / dataset_name / timestamp
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print(f"âš¡ å¿«é€Ÿè¯„ä¼°: {description or timestamp}")
    print("="*80)
    
    # æ˜¾ç¤ºä¿®æ”¹çš„å‚æ•°
    if config_override:
        print("\nä¿®æ”¹çš„å‚æ•°:")
        for key, value in config_override.items():
            old = BASELINE_CONFIG.get(key, 'N/A')
            print(f"  {key}: {old} â†’ {value}")
    
    print(f"\nä½¿ç”¨é¢„å¤„ç†ç¼“å­˜: {cache_dir}")
    print("ä½¿ç”¨ä¸²è¡Œæ¨¡å¼...")

    try:
        # ä¸²è¡Œæ’å€¼
        interpolation_with_params(
            str(located_file),
            str(radar_file),
            str(output_dir),
            update_distance=config['update_distance'],
            update_speed=config['update_speed'],
            update_angle=config['update_angle'],
            ransac_distance=config['ransac_distance'],
            ransac_min_points=config['ransac_min_points'],
            iteration_rounds=config['iteration_rounds'],
            enable_ransac=config['enable_ransac'],
            interpolation_method=config['interpolation_method'],
            enable_kinematic_constraints=config['enable_kinematic_constraints'],
            enable_outlier_detection=config.get('enable_outlier_detection', False)
        )

        # è¯„æµ‹
        score = None
        num_points = 0
        paths = get_dataset_paths(dataset_name)

        result_file = output_dir / 'results.csv'
        if result_file.exists():
            results_df = pd.read_csv(result_file)
            num_points = len(results_df)

            if paths['gt_dir']:
                print("\nè¯„æµ‹ä¸­...")
                score, _ = evaluate(
                    str(result_file),
                    paths['gt_dir'],
                    use_preliminary=True
                )

        # ä¿å­˜å®éªŒè®°å½•
        result = {
            'timestamp': timestamp,
            'description': description,
            'score': score,
            'num_points': num_points,
            'output_dir': str(output_dir),
        }

        # æ·»åŠ é…ç½®å‚æ•°
        for key, value in config.items():
            result[f'config_{key}'] = value

        # ä¿å­˜åˆ°æ—¥å¿—
        log_file = Path('fast_tune_results') / dataset_name / 'experiments.csv'
        df = pd.DataFrame([result])
        if log_file.exists():
            existing = pd.read_csv(log_file)
            df = pd.concat([existing, df], ignore_index=True)
        df.to_csv(log_file, index=False)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*80)
        print("âœ… å®Œæˆï¼")
        print("="*80)
        if score is not None:
            print(f"å¾—åˆ†: {score:.2f}")
        else:
            print(f"å¾—åˆ†: N/A (æ— ground truth)")
        print(f"è½¨è¿¹ç‚¹æ•°: {num_points:,}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")

        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³å¾—åˆ†
        if score and log_file.exists():
            all_results = pd.read_csv(log_file)
            if 'score' in all_results.columns:
                best_score = all_results['score'].max()
                if score >= best_score:
                    print(f"\nğŸ† æ–°çš„æœ€ä½³å¾—åˆ†ï¼")
                    # ä¿å­˜æœ€ä½³é…ç½®
                    best_config_file = Path('fast_tune_results') / dataset_name / 'best_config.json'
                    best = {
                        'score': score,
                        'config': config,
                        'timestamp': timestamp,
                        'description': description
                    }
                    with open(best_config_file, 'w', encoding='utf-8') as f:
                        json.dump(best, f, indent=2, ensure_ascii=False)

        return score, result

    except Exception as e:
        print(f"\nâŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def grid_search(dataset_name, param_grid):
    """ç½‘æ ¼æœç´¢"""
    import itertools

    keys = list(param_grid.keys())
    values = list(param_grid.values())
    combinations = list(itertools.product(*values))

    print(f"\nå¼€å§‹ç½‘æ ¼æœç´¢: {len(combinations)} ä¸ªç»„åˆ")

    results = []
    for i, combo in enumerate(combinations, 1):
        config_override = dict(zip(keys, combo))
        desc = ', '.join([f"{k}={v}" for k, v in config_override.items()])

        print(f"\n{'='*80}")
        print(f"å®éªŒ {i}/{len(combinations)}")
        print("="*80)

        score, result = fast_eval(dataset_name, config_override, desc)
        if score:
            results.append((score, config_override))

    # æ˜¾ç¤ºç»“æœ
    if results:
        results.sort(reverse=True)
        print(f"\n{'='*80}")
        print("ğŸ† ç½‘æ ¼æœç´¢ç»“æœ (æŒ‰å¾—åˆ†æ’åº)")
        print("="*80)
        for i, (score, config) in enumerate(results[:5], 1):
            print(f"\nç¬¬{i}å: {score:.2f}")
            for k, v in config.items():
                print(f"  {k}: {v}")

    return results


def show_history(dataset_name, top_n=10):
    """æ˜¾ç¤ºå†å²å®éªŒ"""
    log_file = Path('fast_tune_results') / dataset_name / 'experiments.csv'

    if not log_file.exists():
        print("è¿˜æ²¡æœ‰å®éªŒè®°å½•")
        return

    df = pd.read_csv(log_file)
    df = df.sort_values('score', ascending=False)

    print("="*80)
    print(f"ğŸ“Š å®éªŒå†å²: {dataset_name}")
    print("="*80)
    print(f"\næ€»å®éªŒæ•°: {len(df)}")
    if 'score' in df.columns and not df['score'].isna().all():
        print(f"æœ€é«˜å¾—åˆ†: {df['score'].max():.2f}")
        print(f"æœ€ä½å¾—åˆ†: {df['score'].min():.2f}")
        print(f"å¹³å‡å¾—åˆ†: {df['score'].mean():.2f}")

    print(f"\nå‰{top_n}å:")
    cols = ['timestamp', 'description', 'score', 'num_points']
    available_cols = [c for c in cols if c in df.columns]
    print(df[available_cols].head(top_n).to_string(index=False))

    return df


def main():
    parser = argparse.ArgumentParser(description='v3.4 å¿«é€Ÿè°ƒä¼˜è„šæœ¬')

    # æ•°æ®é›†é€‰æ‹©
    parser.add_argument('--dataset', default='validation', choices=['validation', 'final'],
                       help='æ•°æ®é›†åç§°')

    # é¢„å¤„ç†ç¼“å­˜
    parser.add_argument('--prepare', metavar='DATASET', help='ç”Ÿæˆé¢„å¤„ç†ç¼“å­˜')

    # å•å‚æ•°æµ‹è¯•
    parser.add_argument('--update_distance', type=int, help='update_distanceå‚æ•°')
    parser.add_argument('--update_speed', type=int, help='update_speedå‚æ•°')
    parser.add_argument('--update_angle', type=int, help='update_angleå‚æ•°')
    parser.add_argument('--ransac_distance', type=int, help='ransac_distanceå‚æ•°')
    parser.add_argument('--ransac_min_points', type=int, help='ransac_min_pointså‚æ•°')
    parser.add_argument('--iteration_rounds', type=int, help='iteration_roundså‚æ•°')
    parser.add_argument('--interpolation_method', help='æ’å€¼æ–¹æ³•: linear/akima')

    # ç½‘æ ¼æœç´¢
    parser.add_argument('--grid', help='ç½‘æ ¼æœç´¢ï¼Œå¦‚: update_distance=2000,2500,3000')

    # æŸ¥çœ‹å†å²
    parser.add_argument('--history', action='store_true', help='æŸ¥çœ‹å®éªŒå†å²')

    # æè¿°
    parser.add_argument('--desc', help='å®éªŒæè¿°')

    args = parser.parse_args()

    # ç”Ÿæˆé¢„å¤„ç†ç¼“å­˜
    if args.prepare:
        prepare_cache(args.prepare)
        return

    # æŸ¥çœ‹å†å²
    if args.history:
        show_history(args.dataset)
        return

    # ç½‘æ ¼æœç´¢
    if args.grid:
        param_grid = {}
        for item in args.grid.split():
            key, vals = item.split('=')
            vals = vals.split(',')
            # å°è¯•è½¬æ¢ä¸ºæ•°å­—
            try:
                vals = [int(v) for v in vals]
            except:
                try:
                    vals = [float(v) for v in vals]
                except:
                    pass
            param_grid[key] = vals

        grid_search(args.dataset, param_grid)
        return

    # å•æ¬¡å¿«é€Ÿè¯„ä¼°
    config_override = {}

    if args.update_distance is not None:
        config_override['update_distance'] = args.update_distance
    if args.update_speed is not None:
        config_override['update_speed'] = args.update_speed
    if args.update_angle is not None:
        config_override['update_angle'] = args.update_angle
    if args.ransac_distance is not None:
        config_override['ransac_distance'] = args.ransac_distance
    if args.ransac_min_points is not None:
        config_override['ransac_min_points'] = args.ransac_min_points
    if args.iteration_rounds is not None:
        config_override['iteration_rounds'] = args.iteration_rounds
    if args.interpolation_method is not None:
        config_override['interpolation_method'] = args.interpolation_method

    if not config_override:
        # é»˜è®¤ï¼šè¿è¡Œbaseline
        print("è¿è¡Œbaselineé…ç½®...")
        config_override = {}

    fast_eval(args.dataset, config_override, args.desc or "")


if __name__ == '__main__':
    main()


