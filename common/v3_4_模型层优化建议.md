# v3.4 模型层优化分析与方向

## 一、模型层面现在相对“朴素”的地方

### 1. 插值 / 轨迹建模：只有 Akima 插值，没有真正的动力学模型

从 v3.4 的入口来看，当前整体是：

- 几何定位 + 阈值匹配 + Akima 插值 + RANSAC 扩展 + 简单运动学 / 离群点约束。

好处：

- Akima 插值平滑，比简单线性 / 样条更鲁棒；

但目前**没有显式状态（位置、速度、加速度）**，也没有“预测-更新”的过程：

- 没有用“上一刻的速度 / 加速度”来对下一刻位置做贝叶斯预测；
- 运动学约束和离群点检测多半是**阈值规则**，而不是在统一框架里。

这会导致两个比较典型的现象（在稀疏 + 有偏的复赛数据上会更明显）：

1. 对缺失比较长的时间段：插值可能更多是“按几何来”，而不是按物理运动来；
2. 对局部偏差 / 噪声：只能通过 RANSAC 和简单 outlier 清理，不能很智能地“信哪一个传感器”。

换句话说：插值层面还停留在“几何拟合 + 规则过滤”，**没有上到简易卡尔曼 / RTS 平滑**这一层级。

---

### 2. 点关联：局部阈值 + 贪心匹配，没有全局数据关联

从参数命名（`update_distance` / `update_speed` / `update_angle`）和消融脚本来看，v3.4 的点匹配逻辑大致是：

- 在每轮迭代里，对“未定位雷达点”和“当前轨迹末端”之间，用距离 / 速度 / 转角做打分或筛选；
- 采用某种**局部贪心匹配**：选满足阈值中“最优”的那个，或者按顺序更新。

这种做法天然有一些“普通化”的特点：

1. **每条船、每一时刻局部做决策**，缺少“全局最优”的考虑：
   - 多条航迹彼此抢点时，容易出现“局部最优但整体错配”的情况；
2. **没有用到“概率 / 权重”级的数据关联**（类似 JPDA 那种）：
   - 距离 / 速度 / 角度目前扮演的是“硬阈值 + 排序”的角色，
   - 而不是“构造一个统一的代价函数，然后全局求最优匹配”。

从模型视角看，**数据关联层还是基于规则的 greedy**，并没有用图模型 / 匈牙利算法 / 概率关联等更“模型化”的方法。

---

### 3. 运动学约束 & 离群点：rule-based，而不是“轨迹级一致性优化”

当前已经有：

- `enable_kinematic_constraints`（速度 / 加速度约束）；
- `enable_outlier_detection`（插值前清理异常点）；

但从接口形态推测，很可能是：

- 对单点或局部窗口做阈值检测；
- 不会对**整条轨迹**做 joint 优化（例如：在 global cost 下整体调整一条轨迹）。

典型的“还可以更聪明”的地方包括：

- 基于整条轨迹的速度 / 加速度分布，统一拟合一个更平滑的速度曲线，再强制位置轨迹与之吻合；
- 对某些离群点，不仅是删掉它，而是考虑“换一组点重新插值”这样的全局调整。

这些都属于“模型整体一致性”的范畴，目前 pipeline 更像是：**先插值，再用规则修一修**。

---

## 二、在不大改结构的前提下，几件“显而易见能优化”的方向

前提：不推翻 v3.4，只在模型层比现在更高级一点，工程侵入性尽量小（不包含已经做过的“参数调优 + 偏差显式偏爱”）。

### 1. 加一层“统一的轨迹后处理模块”（速度 / 加速度平滑 + 缺点补全）

可以做成一个**独立模块**，在 `interpolation_with_params` 之后、写 `results.csv` 之前 / 之后调用，几乎不动现有主逻辑：

- 对每艘船的轨迹：
  - 基于位置序列，估一个速度、转向角序列；
  - 对速度 / 转向角做平滑（移动平均 / Gaussian / Savitzky-Golay 等）；
  - 再用平滑后的速度 / 转角，反推回位置（或对原位置做小幅修正）。
- 对时间轴上“偶发断点 / 漏点”，可以做一个简单的“局部再插值”，确保：
  - 时间严格 1 min 间隔；
  - 不出现极端跳变。

这种“结果级 post-process”的好处：

- 和现有代码完全解耦，实在不行可以一行开关关掉；
- 很契合复赛宽容公式：**宁可稍微糊一点，也要稳定、连续**。

---

### 2. 把“迭代轮数 / 匹配强度”做成自适应，而不是固定 5 轮

现在 `iteration_rounds = 5` 是固定的，但可以改成自适应：

- 每条轨迹记录“这一轮真正新增的点数”；
- 当连续几轮新增很少（或收敛到 0）时，提前停止；
- 对于点数很多、雷达很密的船，可以允许更多轮迭代；
- 对于点很少的，就避免过拟合。

这样可以让算法的“工作量”和“数据难度”自适应，而不是统一敲 5 下，更有“模型感”。

---

### 3. 改造点匹配评分：从“多重阈值”变成一个统一的代价函数

在不上 JPDA / 匈牙利算法的前提下，可以做一个折中版本：

- 给每个候选匹配点算一个综合 cost，例如：

  ```text
  cost = w_d * (distance / scale_d)^2 + w_v * (speed_diff / scale_v)^2 + w_θ * (angle_diff / scale_θ)^2
  ```

- 然后在 cost 最小的一两个里面再加一点规则判断；
- 再配合一个 soft 的阈值（比如 `cost < 某个值`），减少“虽然某一项稍超阈值，但总体还不错”的误杀。

好处：

- 依然是局部贪心，但“贪的是一个统一意义上的最优”，比现在“先按距离、再看速度、再看角度”这种多层 if 更稳；
- 改动只发生在 `ablation_wrapper` 的匹配函数里，不动入口脚本。

---

### 4. 针对 5 号 ESM 的“显式偏爱”，与匹配 cost 结合

“偏差显式偏爱”很自然可以和上面的 cost 模型绑在一起：

- 在 cost 里加一项与传感器类型相关的 penalty / weight：
  - 来自 5 号 ESM 的观测：给更低的 noise weight；
  - 来自有偏传感器的观测：给更高的 noise，使它们“自然地不那么有话语权”。

这样不需要立刻把偏差数值估出来，也能在模型里“显式偏爱” 5 号 / 低噪声传感器。

---

## 三、一句话总结

- 从 `run_optimized_v3_4.py` 来看，当前方案已经是一个“几何 + Akima + RANSAC + 一点规则约束”的成熟工程版 baseline，并不是很初级的玩具；
- 但模型层面仍偏传统 / 朴素：
  - 没有状态空间模型（卡尔曼 / 平滑）；
  - 点关联是局部阈值 greedy；
  - 运动学约束和离群点主要靠规则，并没有做轨迹级统一优化。

在不大改结构的前提下，优先可以考虑：

1. 加一个独立的“轨迹后处理模块”（速度平滑 + 漏点补全）；
2. 把迭代轮数和匹配力度做成自适应；
3. 在匹配里用统一的代价函数 + 传感器权重，弱化硬阈值。

